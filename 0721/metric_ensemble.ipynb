{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Metric Learning\n",
    "--------------\n",
    "## Mahalanobis Metric Learning for Clustering (MMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import metric_learn\n",
    "from metric_learn import MMC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate random data with multiple clusters\n",
    "n_samples = 40\n",
    "random_state = 170\n",
    "X, y = make_blobs(centers = 2)\n",
    "\n",
    "# First, we obtain labels (0 or 1) for this data set\n",
    "kmeans = KMeans(n_clusters = 2)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualization of Kmeans clustering\n",
    "plt.scatter(X[:,0], X[:,1], c = y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [문제] 주어진 데이터로 MMC를 학습시킨 후에, 변형시키시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the labels:\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Find the indices corresponding to each label (0 or 1)\n",
    "A = np.where(kmeans.labels_ == 1)\n",
    "B = np.where(kmeans.labels_ == 0)\n",
    "\n",
    "label_1 = A[0]\n",
    "label_0 = B[0]\n",
    "\n",
    "# Now, data corresponding to indices stored in A are SIMILAR to each other\n",
    "# data corresponding to indices stored in A and B are DISSIMILAR to each other\n",
    "ind_1 = np.concatenate((label_1[0:4], label_1[10:14]), axis=None)\n",
    "ind_2 = np.concatenate((label_1[5:9], label_0[0:4]), axis=None)\n",
    "pair = [X[ind_1], X[ind_2]]\n",
    "pair = np.stack(pair, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform MMC transformation\n",
    "### YOUR CODE HERE (Fill in the \"None\")\n",
    "# Hint: define a MMC object of 200 iterations\n",
    "mmc = None\n",
    "mmc.fit(None, [1]*4+[-1]*4)\n",
    "\n",
    "# Transform the data using the MMC we just fitted\n",
    "X_transformed = None\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the transformed data\n",
    "plt.scatter(X_transformed[:,0], X_transformed[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ensemble Learning\n",
    "--------------\n",
    "## AdaBoostClassifier (Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libaray function\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "# For votingclassifier ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Support vector machine classifier - details in https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "from sklearn.svm import SVC\n",
    "# StandardScaler for scaling the dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_path = 'dataset/datasets_diabetes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the dataset\n",
    "df = pd.read_csv(Dataset_path)\n",
    "#take a look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X : dataset feature, y : dataset label\n",
    "X = df.drop(columns = ['Outcome'])\n",
    "y = df['Outcome']\n",
    "# normalize dataset for training\n",
    "X = StandardScaler().fit_transform(X)\n",
    "#split data into train and test sets - Ratio : 4:1 (4 for training, 1 for test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)# X : dataset feature, y : dataset label\n",
    "X = df.drop(columns = ['Outcome'])\n",
    "y = df['Outcome']\n",
    "# normalize dataset for training\n",
    "X = StandardScaler().fit_transform(X)\n",
    "#split data into train and test sets - Ratio : 4:1 (4 for training, 1 for test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [문제] 주어진 Training data를 AdaBoostClassifier로 학습하고 가장 높은 정확도를 가지는 estimator number를 출력하시오. 그리고 최적의 estimator 수를 사용한 AdaBoostClassifier의  feature importance를 그래프로 그리시오.\n",
    "1. estimator의 수에 따른 prediction accuracy의 변화를 그리시오. \n",
    "2. Best estimator number와 Best prediction accuracy를 출력하시오. \n",
    "3. Best estimator number를 사용한 AdaBoostClassifier의 feature importance를 그래프로 그리시오. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the estimator range\n",
    "estimator = list(range(1,101))\n",
    "# To find best number of estimator, initialize best_estimator and max_score, and best_model\n",
    "best_estimator = -1\n",
    "max_score = -1\n",
    "best_model = None\n",
    "# To plot the trend of score along the estomator, initialize the list of score\n",
    "score_list = []\n",
    "for n_estimators in estimator :\n",
    "    # Create AdaBoostClassifier with n_estimators.\n",
    "    # Hint: The number of estimators are configured by 'n_estimators' parameter .\n",
    "    # Hint: We use 0 as seed which is given at each base_estimator at each boosting iteratio n.\n",
    "    AdaBoost_clf = None\n",
    "    k_folds = KFold(n_splits=20)\n",
    "    results = cross_val_score(AdaBoost_clf, X_train, y_train, cv=k_folds)\n",
    "    score = results.mean()\n",
    "    score_list.append(score)\n",
    "    if score > max_score :\n",
    "        max_score = score\n",
    "        best_estimator = n_estimators\n",
    "        best_model = AdaBoost_clf\n",
    "plt.plot(estimator, score_list)\n",
    "plt.xlabel('Number of estimators')\n",
    "plt.ylabel('Prediction accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best estimator number and the maximum score\n",
    "print(\"Best Estimator number : {} / Best prediction accuracy : {}\".format(None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AdaBoostClassifier\n",
    "# Hint: the number of estimators giving the best performance is best_estimator.\n",
    "AdaBoost_clf = None\n",
    "AdaBoost_clf.fit(X_train, y_train)\n",
    "fig, ax = plt.subplots()\n",
    "x=list(df.columns[:-1])\n",
    "# Plot AdaBoost classifier's feature importances\n",
    "ax.plot_date(x, None, 'bo')\n",
    "fig.autofmt_xdate()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
